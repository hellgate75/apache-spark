spark.master=spark://MASTER_HOST:MASTER_PORT
spark.eventLog.enabled=EVENT_LOG_ENABLED
spark.eventLog.dir=hdfs://localhost:8021/directory
spark.serializer=SPARK_SERIALIZER_CLASS
spark.driver.memory=SPARK_DRIVER_MEMORY
spark.executor.extraJavaOptions=SPARK_EXTRA_JVM_OPTIONS
#Enable periodic cleanup of worker / application directories. Note that this only affects standalone mode, as YARN works differently. Only the directories of stopped applications are cleaned up.
spark.worker.cleanup.enabled=CLEAN_UP_ENABLED
#Controls the interval, in seconds, at which the worker cleans up old application work dirs on the local machine.
spark.worker.cleanup.interval=CLEAN_UP_INTERVAL
#The number of seconds to retain application work directories on each worker. This is a Time To Live and should depend on the amount of available disk space you have. Application logs and jars are downloaded to each application work dir. Over time, the work dirs can quickly fill up disk space, especially if you run jobs very frequently.
spark.worker.cleanup.appDataTtl=CLEAN_UP_TTL
#For compressed log files, the uncompressed file can only be computed by uncompressing the files. Spark caches the uncompressed file size of compressed log files. This property controls the cache size.
spark.worker.ui.compressedLogFileLengthCacheSize=COMPRESSED_CACHED_SIZE
